5c738de7c fix: update terminal settings documentation link (#8997)
5fb36d9c8 feat: implement Google Consent Mode v2 with cookieless pings (#8987)
34f45f1b2 feat: add OpenRouter embedding provider support (#8973)
d0e519de3 feat: convert Chutes to dynamic/router provider (#8980)
ca10cba3c fix: create new Requesty profile during OAuth (#8699)
25c6f46e7 fix: prevent message loss during queue drain race condition (#8955)
89d67efc6 fix: add keyword index for type field to fix Qdrant codebase_search error (#8964)
81d5f63b7 fix: remove search_and_replace tool from codebase (#8892)
c66fed961 feat: improve @ file search for large projects (#8805)
b284eddf4 fix: prevent infinite loop when canceling during auto-retry (#8902)
f583be3c3 fix(context): truncate type definition to match max read line (#8509)
9dfa49665 fix: make code index initialization non-blocking at activation (#8933)
bd5807bac fix: Gate auth-driven Roo model refresh to active provider only (#8915)
414345ce7 Feat: Add Minimax Provider (fixes #8818) (#8820)
f839d4c27 fix: prevent MCP server restart when toggling tool permissions (#8633)
82b0b04b5 docs(vscode-lm): clarify VS Code LM API integration warning (#8493)
06af804c2 fix(modes): custom modes under custom path not showing (#8499)
06a5c2dd1 fix: auto-sync enableReasoningEffort with reasoning dropdown selection (#8890)
be119bcc5 Add exponential backoff for mid-stream retry failures (#8888)
49fde4161 feat: update Gemini models with latest 09-2025 versions (#8486)
bde2c3cec fix: use max_output_tokens when available in LiteLLM fetcher (#8455)
13d20bbe8 fix: process queued messages after context condensing completes (#8478)
cff65b4f5 fix: resolve checkpoint menu popover overflow (#8867)
634b1df17 fix: Remove specific Claude model version from settings descriptions (#8437)
8870348bd feat: add Google Ads conversion tracking to reviewer page (#8831)
f7009e9b9 fix: change Add to Context keybinding to avoid Redo conflict (#8653)
98b8d5b00 fix: adjust GLM-4.6-turbo max output tokens to prevent context limit errors (#8822)
aae255d1b fix(export): exclude max tokens field for models that don't support it (#8464)
a84f7ef77 fix: respect nested .gitignore files in search_files (#8804)
ad56791c3 fix: preserve trailing newlines in stripLineNumbers for apply_diff (#8227)
f4121e256 Add checkpoint initialization timeout settings and fix checkpoint timeout warnings (#8019)
4f084f89e fix: retry API requests on stream failures instead of aborting task (#8794)
b9110dc24 fix: always show checkpoint restore options regardless of change detection (#8758)
8187a8e18 fix: update X/Twitter username from roo_code to roocode (#8780)
34392dd4d Z.ai: add GLM-4.5-X, AirX, Flash (expand model coverage) (#8745)
270dce505 fix(editor): prevent file editing issues when git diff views are open (#8676)
6b8c21f87 fix(i18n): Update zh-TW run command title (#8631)
960958a6d fix: add ollama and lmstudio to MODELS_BY_PROVIDER (#8511)
38c802852 fix: properly reset cost limit tracking when user clicks "Reset and Continue" (#6890)
1de480b38 fix: improve save button activation in prompts settings (#5780) (#8267)
9bcd9918a fix: Addresses overeager 'there are unsaved changes' dialog in settings (#8410)
3e47e88c0 fix: show send button when only images are selected in chat textarea (#8423)
9af9d5b02 A couple more sonnet 4.5 fixes (#8421)
7b7bb4957 fix: remove topP parameter from Bedrock inference config (#8388)
b26449575 fix: Anthropic Sonnet 4.5 model id + Bedrock 1M context checkbox (#8384)
675968718 fix: correct Claude Sonnet 4.5 model ID format (#8373)
c57539d4b fix: correct AWS Bedrock Claude Sonnet 4.5 model identifier (#8372)
602901fa0 fix: use max_completion_tokens for GPT-5 models in LiteLLM provider (#6980)
5e218febd fix: remove <thinking> tags from prompts for cleaner output and fewer tokens (#8319)
2f1b94f43 fix: include initial ask in condense summarization (#8293) (#8298)
