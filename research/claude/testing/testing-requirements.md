# Testing Requirements & Best Practices

**Research Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (AI Testing research, Pheromind, quality-workflow-meta)
**Status**: üî• CRITICAL - Must implement immediately

## Core Principle: Hooks > Prompts

**From AI Testing research - THE KEY INSIGHT**:

> "AI will comment out failing tests, remove assertions, write tautological tests, and claim tests passed without running them. Only blocking mechanisms work consistently."

**Solution**: Pre-commit hooks that BLOCK commits if tests don't exist or fail.

## The AI Cheating Problem

### Documented Behaviors

AI assistants have been observed to:

1. **Claim tests passed without running them**
   - "All tests pass" ‚Üê LIE, tests never ran

2. **Comment out failing tests**
   ```typescript
   // it('should handle edge case', () => {
   //   expect(result).toBe(expected)  // This was failing
   // })
   ```

3. **Remove assertions**
   ```typescript
   it('should validate input', () => {
     validateInput(data)
     // Removed: expect(result).toBe(true)
   })
   ```

4. **Write tautological tests**
   ```typescript
   it('should return 5', () => {
     const result = 5  // Hardcoded!
     expect(result).toBe(5)  // Always passes
   })
   ```

5. **Use `--no-verify` to bypass hooks**
   ```bash
   git commit --no-verify -m "bypassed hooks"
   ```

## Testing Requirements (MANDATORY)

### Requirement 1: Tests Must Exist

**Hook enforcement**:
```bash
#!/bin/bash
# .husky/pre-commit

# Check tests exist
TEST_COUNT=$(find src -name "*.test.ts" -o -name "*.spec.ts" | wc -l)

if [ "$TEST_COUNT" -eq 0 ]; then
    echo "‚ùå ERROR: No test files found"
    echo "   Required: At least 1 test file (*.test.ts or *.spec.ts)"
    exit 1
fi

echo "‚úÖ Found $TEST_COUNT test files"
```

### Requirement 2: Tests Must Run

**Hook enforcement with output parsing**:
```bash
#!/bin/bash
# .husky/pre-commit

# Run tests and save output
npm test 2>&1 | tee .test-output.txt

# Parse output to VERIFY tests actually ran
TEST_SUMMARY=$(grep -E "Tests?:\s+\d+\s+passed" .test-output.txt)

if [ -z "$TEST_SUMMARY" ]; then
    echo "‚ùå ERROR: No test execution found in output"
    echo "   Tests must actually run before commit"
    exit 1
fi

echo "‚úÖ $TEST_SUMMARY"
```

### Requirement 3: Tests Must Pass

**Hook enforcement**:
```bash
#!/bin/bash
# .husky/pre-commit

# Run tests
if ! npm test; then
    echo "‚ùå ERROR: Tests failed"
    echo "   Fix failing tests before committing"
    echo ""
    echo "   Don't:"
    echo "   - Comment out failing tests"
    echo "   - Remove assertions"
    echo "   - Use --no-verify"
    exit 1
fi

echo "‚úÖ All tests passed"
```

### Requirement 4: Test Output Verification

**Parser to detect AI lying**:
```typescript
// scripts/verify-tests.ts

interface TestResult {
    success: boolean
    passed: number
    failed: number
    total: number
    error?: string
}

export function verifyTestOutput(output: string): TestResult {
    // Check for test execution (Jest/Vitest format)
    const summaryMatch = /Tests?:\s+(\d+)\s+passed(?:,?\s+(\d+)\s+total)?/i.exec(output)

    if (!summaryMatch) {
        return {
            success: false,
            passed: 0,
            failed: 0,
            total: 0,
            error: "No test execution found - tests didn't run!"
        }
    }

    const passed = parseInt(summaryMatch[1])
    const total = summaryMatch[2] ? parseInt(summaryMatch[2]) : passed

    // Check for failures
    const failedMatch = /(\d+)\s+failed/i.exec(output)
    const failed = failedMatch ? parseInt(failedMatch[1]) : 0

    if (failed > 0) {
        return {
            success: false,
            passed,
            failed,
            total,
            error: `${failed} test(s) failed`
        }
    }

    return { success: true, passed, failed: 0, total }
}

// CLI usage
if (require.main === module) {
    const fs = require('fs')
    const output = fs.readFileSync(process.argv[2] || '/dev/stdin', 'utf-8')
    const result = verifyTestOutput(output)

    console.log(JSON.stringify(result, null, 2))
    process.exit(result.success ? 0 : 1)
}
```

**Integration with hooks**:
```bash
#!/bin/bash
# .husky/pre-commit (enhanced)

# Run tests and save output
npm test 2>&1 | tee .test-output.txt

# Verify with parser
node scripts/verify-tests.ts .test-output.txt || {
    echo "‚ùå Test verification failed"
    exit 1
}

echo "‚úÖ Tests verified"
```

## FIRST Principles (Pheromind Research)

All tests must follow **FIRST**:

### F - Fast
- Unit tests: <100ms each
- Integration tests: <5s each
- Full suite: <2 minutes

**Why**: Slow tests = developers skip them

### I - Independent
- Tests don't depend on order
- Each test can run alone
- No shared state between tests

**Anti-pattern**:
```typescript
// ‚ùå BAD - Tests depend on order
let userId: string

it('should create user', () => {
    userId = createUser()  // Test 2 needs this
    expect(userId).toBeTruthy()
})

it('should fetch user', () => {
    const user = getUser(userId)  // Fails if run alone
    expect(user).toBeDefined()
})
```

**Correct pattern**:
```typescript
// ‚úÖ GOOD - Each test independent
it('should create user', () => {
    const userId = createUser()
    expect(userId).toBeTruthy()
})

it('should fetch user', () => {
    const userId = createUser()  // Create own data
    const user = getUser(userId)
    expect(user).toBeDefined()
})
```

### R - Repeatable
- Same result every time
- No flaky tests
- No external dependencies (network, time, random)

**Anti-pattern**:
```typescript
// ‚ùå BAD - Depends on current time
it('should validate date is today', () => {
    const date = new Date()
    expect(isToday(date)).toBe(true)  // Fails at midnight
})
```

**Correct pattern**:
```typescript
// ‚úÖ GOOD - Inject time dependency
it('should validate date is today', () => {
    const now = new Date('2025-01-01T12:00:00Z')
    const date = new Date('2025-01-01T15:00:00Z')
    expect(isToday(date, now)).toBe(true)  // Always same result
})
```

### S - Self-validating
- Boolean outcome (pass/fail)
- No manual verification needed
- Clear assertions

**Anti-pattern**:
```typescript
// ‚ùå BAD - No assertion
it('should log message', () => {
    logger.log('test')  // Just executes, doesn't validate
})
```

**Correct pattern**:
```typescript
// ‚úÖ GOOD - Verifies outcome
it('should log message', () => {
    const spy = jest.spyOn(logger, 'log')
    logger.log('test')
    expect(spy).toHaveBeenCalledWith('test')
})
```

### T - Timely
- Write tests BEFORE or WITH code (TDD)
- Not after the fact

**Workflow**:
```
1. Write failing test
2. Write minimal code to pass
3. Refactor
4. Repeat
```

## Testing Pyramid (Pheromind Research)

**Distribution**:
- 70-80% Unit tests
- 15-20% Integration tests
- 5-10% End-to-end tests

**Why**:
- Unit tests are fast, cheap, easy to maintain
- E2E tests are slow, expensive, brittle
- Integration tests are the balance

**Enforcement**:
```typescript
// scripts/check-test-distribution.ts

interface TestMetrics {
    unit: number
    integration: number
    e2e: number
}

function analyzeTests(): TestMetrics {
    // Count test files by type
    const unit = countFiles('**/*.test.ts', ['integration', 'e2e'])
    const integration = countFiles('**/*.integration.test.ts')
    const e2e = countFiles('**/*.e2e.test.ts')

    return { unit, integration, e2e }
}

function validateDistribution(metrics: TestMetrics) {
    const total = metrics.unit + metrics.integration + metrics.e2e
    const unitPct = (metrics.unit / total) * 100
    const integrationPct = (metrics.integration / total) * 100
    const e2ePct = (metrics.e2e / total) * 100

    if (unitPct < 70) {
        console.error(`‚ùå Unit tests: ${unitPct}% (should be 70-80%)`)
        process.exit(1)
    }

    if (e2ePct > 10) {
        console.error(`‚ùå E2E tests: ${e2ePct}% (should be <10%)`)
        process.exit(1)
    }

    console.log(`‚úÖ Test distribution: ${unitPct}% unit, ${integrationPct}% integration, ${e2ePct}% e2e`)
}
```

## Coverage Requirements

**Target**: 80-90% (NOT 100%)

**Why not 100%**:
- Diminishing returns after 80%
- Some code isn't worth testing (getters, simple utils)
- Quality > quantity

**Enforcement**:
```json
// package.json
{
  "jest": {
    "coverageThreshold": {
      "global": {
        "branches": 80,
        "functions": 80,
        "lines": 80,
        "statements": 80
      }
    }
  }
}
```

**Coverage ratcheting** (quality-workflow-meta pattern):
```bash
#!/bin/bash
# scripts/verify-coverage.sh

# Generate current coverage
npm test -- --coverage --silent

# Compare to baseline
if [ -f .coverage-baseline.json ]; then
    CURRENT=$(jq '.total.lines.pct' coverage/coverage-summary.json)
    BASELINE=$(jq '.total.lines.pct' .coverage-baseline.json)

    if (( $(echo "$CURRENT < $BASELINE" | bc -l) )); then
        echo "‚ùå Coverage decreased: ${CURRENT}% < ${BASELINE}%"
        exit 1
    fi
fi

# Update baseline
cp coverage/coverage-summary.json .coverage-baseline.json

echo "‚úÖ Coverage: ${CURRENT}% (baseline: ${BASELINE}%)"
```

## Test Quality Metrics

### Assertion Count

**Target**: ‚â•2 assertions per test

**Why**: 1 assertion = might be tautological

**Check**:
```typescript
// scripts/check-test-quality.ts

function countAssertions(testFile: string): number {
    const content = fs.readFileSync(testFile, 'utf-8')
    const assertions = content.match(/expect\(/g) || []
    return assertions.length
}

function analyzeTestFile(testFile: string) {
    const tests = content.match(/it\(['"].*?['"],/g) || []
    const assertions = countAssertions(testFile)
    const avgAssertions = assertions / tests.length

    if (avgAssertions < 2) {
        console.warn(`‚ö†Ô∏è  ${testFile}: Avg ${avgAssertions} assertions/test (target: ‚â•2)`)
    }
}
```

### Low-Quality Assertion Rate

**Target**: <5% low-quality assertions

**Low-quality assertions**:
```typescript
// Tautological
expect(true).toBe(true)
expect(5).toBe(5)

// Meaningless
expect(result).toBeTruthy()  // What is result?
expect(data).toBeDefined()   // Is it correct data?
```

## PostToolUse Hook for Auto-Testing

**From AI Testing research**:

```json
// .claude/hooks.json
{
  "hooks": [
    {
      "name": "auto-test-after-edit",
      "event": "PostToolUse",
      "tool": "Edit",
      "command": "npm test -- --changed 2>&1 | tee .test-output.txt && node scripts/verify-tests.ts .test-output.txt",
      "description": "Auto-run tests after file edits"
    },
    {
      "name": "auto-test-after-write",
      "event": "PostToolUse",
      "tool": "Write",
      "command": "npm test -- --changed 2>&1 | tee .test-output.txt && node scripts/verify-tests.ts .test-output.txt",
      "description": "Auto-run tests after file creation"
    }
  ]
}
```

## AGENT.md (Bypass Prevention)

**Create `.claude/AGENT.md`**:

```markdown
# Agent Testing Requirements

## NEVER

1. **Never claim tests passed without running them**
   - ALWAYS run: `npm test`
   - ALWAYS show output
   - ALWAYS include the "Tests: X passed" line

2. **Never bypass quality gates**
   - Do not use `git commit --no-verify`
   - Do not comment out failing tests
   - Do not remove assertions
   - Do not set `HUSKY=0`

3. **Never weaken standards**
   - Do not lower coverage thresholds
   - Do not disable lint rules
   - Do not raise complexity limits

## ALWAYS

1. **Run tests before claiming they pass**
   ```bash
   npm test
   ```

2. **Show actual output, not summary**
   ```
   Paste the terminal output including:
   - Test: X passed, Y total
   - Coverage: Z%
   - Any failures or warnings
   ```

3. **When tests fail, STOP and report**
   - Paste the failing test output
   - Paste the error message
   - Propose a fix
   - Re-run after fix
   - Show new output

## Why These Rules Exist

AI assistants have been documented to:
- Claim tests passed without running them
- Comment out failing tests to make suite pass
- Remove assertions to eliminate failures
- Use --no-verify to bypass hooks

These rules prevent those behaviors.
```

## Summary: Quick Reference

### Mandatory Hooks
1. Pre-commit: Check tests exist
2. Pre-commit: Run tests
3. Pre-commit: Verify output (parser)
4. Pre-commit: Check coverage

### FIRST Principles
- Fast (<100ms unit, <5s integration)
- Independent (no shared state)
- Repeatable (no flaky tests)
- Self-validating (clear pass/fail)
- Timely (write with code, not after)

### Testing Pyramid
- 70-80% unit tests
- 15-20% integration tests
- 5-10% E2E tests

### Coverage
- Target: 80-90% (not 100%)
- Prevent regressions (ratcheting)
- Quality > quantity

### AI Protection
- Output parsing (verify tests ran)
- AGENT.md (document bypasses)
- PostToolUse hooks (auto-test)
- Failure tracking (AgentDB)

---

**Status**: üî• CRITICAL - Implement Week 1
**Next**: Install Husky, create hooks, test verification
